{
 "metadata": {
  "name": "",
  "signature": "sha256:c7faafd6f7ef49aac0d12bca906453a4fbc50b5119e20f9818ef429f6efbf6cc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import required packages\n",
      "import numpy as np\n",
      "import lda\n",
      "import nltk \n",
      "import gensim\n",
      "from nltk.corpus import stopwords\n",
      "import nltk.data\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "#Import documents\n",
      "with open(\"C:\\\\Users\\\\Anupama\\\\OneDrive\\\\INFX575\\\\Assignment3\\\\abstracts.txt\") as f:\n",
      "    reader = csv.reader(f, delimiter=\"\\t\")\n",
      "    abstracts = list(reader)\n",
      "    \n",
      "#Set up a document dictionary mapping document id to document\n",
      "abstracts_dict = {t[0]:t[1] for t in abstracts}\n",
      "\n",
      "#Import the group mappings\n",
      "with open(\"C:\\\\Users\\\\Anupama\\\\OneDrive\\\\INFX575\\\\Assignment3\\\\groups.txt\") as f:\n",
      "    reader = csv.reader(f, delimiter=\"\\t\")\n",
      "    groups = list(reader)\n",
      "    \n",
      "#Set up a group dictionary mapping document to group number\n",
      "groups_dict = {t[0]:t[1] for t in groups[1:]}\n",
      "\n",
      "#Create the first group of documents. Remove nulls\n",
      "i_list = list(key for key, value in groups_dict.iteritems() if value == '1')\n",
      "i_group = [abstracts_dict[x] for x in i_list]\n",
      "i_group = [doc for doc in i_group if doc != 'null']\n",
      "\n",
      "##Create the second group of documents. Remove nulls\n",
      "j_list = list(key for key, value in groups_dict.iteritems() if value == '2')\n",
      "j_group = [abstracts_dict[x] for x in j_list]\n",
      "j_group = [doc for doc in j_group if doc != 'null']\n",
      "\n",
      "#Create the third group of documents. Remove nulls\n",
      "k_list = list(key for key, value in groups_dict.iteritems() if value == '3')\n",
      "k_group = [abstracts_dict[x] for x in k_list]\n",
      "k_group = [doc for doc in k_group if doc != 'null']\n",
      "\n",
      "#Create a corpus ccomprising of all documents\n",
      "corpus = i_group+j_group+k_group"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Define a stop list\n",
      "stoplist = [\"all\",\"just\",\"being\",\"over\",\"both\",\"through\",\"yourselves\",\"its\",\"before\",\"herself\",\"had\",\"should\",\"to\",\"only\",\"under\",\"ours\",\"has\",\"do\",\"them\",\"his\",\"very\",\"they\",\"not\",\"during\",\"now\",\"him\",\"nor\",\"did\",\"this\",\"she\",\"each\",\"further\",\"where\",\"few\",\"because\",\"doing\",\"some\",\"are\",\"our\",\"ourselves\",\"out\",\"what\",\"for\",\"while\",\"does\",\"above\",\"between\",\"t\",\"be\",\"we\",\"who\",\"were\",\"here\",\"hers\",\"by\",\"on\",\"about\",\"of\",\"against\",\"s\",\"or\",\"own\",\"into\",\"yourself\",\"down\",\"your\",\"from\",\"her\",\"their\",\"there\",\"been\",\"whom\",\"too\",\"themselves\",\"was\",\"until\",\"more\",\"himself\",\"that\",\"but\",\"don\",\"with\",\"than\",\"those\",\"he\",\"me\",\"myself\",\"these\",\"up\",\"will\",\"below\",\"can\",\"theirs\",\"my\",\"and\",\"then\",\"is\",\"am\",\"it\",\"an\",\"as\",\"itself\",\"at\",\"have\",\"in\",\"any\",\"if\",\"again\",\"no\",\"when\",\"same\",\"how\",\"other\",\"which\",\"you\",\"after\",\"most\",\"such\",\"why\",\"a\",\"off\",\"i\",\"yours\",\"so\",\"the\",\"having\",\"once\"]\n",
      "\n",
      "tokenizer = RegexpTokenizer(r'\\w+') #Initialize a tokenizer\n",
      "i_tokens = tokenizer.tokenize(\" \".join(i_group)) #Convert the i group into a flat list of unigrams\n",
      "i_tokens = [word.lower() for word in i_tokens if word not in stoplist] #Remove stop words from the i group\n",
      "\n",
      "j_tokens = tokenizer.tokenize(\" \".join(j_group)) #Convert the j group into a flat list of unigrams\n",
      "j_tokens = [word.lower() for word in j_tokens if word not in stoplist] #Remove stop words from the j group\n",
      "\n",
      "k_tokens = tokenizer.tokenize(\" \".join(k_group)) #Convert the k group into a flat list of unigrams\n",
      "k_tokens = [word.lower() for word in k_tokens if word not in stoplist] #Remove stop words from the k group\n",
      "\n",
      "corpus_tokens = i_tokens+j_tokens+k_tokens #Combine the cleaned up tokens together into a corpus of tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i_dict = {x:i_tokens.count(x) for x in i_tokens} #Create a word frequency dictionary for the i cluster\n",
      "j_dict = {x:j_tokens.count(x) for x in j_tokens} #Create a word frequency dictionary for the j cluster\n",
      "k_dict = {x:k_tokens.count(x) for x in k_tokens} #Create a word frequency dictionary for the k cluster\n",
      "corpus_dict = {x:corpus_tokens.count(x) for x in corpus_tokens} #Create a word frequency dictionary for the whole corpus\n",
      "\n",
      "i_p={} #Create an empty dictionary to store the probability distributions for the words in the i cluster\n",
      "j_p={} #Create an empty dictionary to store the probability distributions for the words in the j cluster\n",
      "k_p={}#Create an empty dictionary to store the probability distributions for the words in the k cluster\n",
      "corpus_p={} #Create an empty dictionary to store the probability distributions for the words in the whole corpus\n",
      "\n",
      "for word in corpus_dict.keys(): #For each word in the corpus\n",
      "    \n",
      "    corpus_p[word] = corpus_dict[word]/float(sum(corpus_dict.values())) #Compute the probability of that word occurring in the corpus\n",
      "    \n",
      "    if word in i_dict: #If the word exists in the i group\n",
      "        i_p[word] = i_dict[word]/float(sum(i_dict.values())) #Compute the probability of that word occurrring in the i group\n",
      "    else: #If the word does not exist in the i group\n",
      "        i_p[word] = 0 #Assign a zero probability\n",
      "    \n",
      "    i_p[word] = 0.99*i_p[word]+0.01*corpus_p[word] #Use the \"fudge factor\" of alpha = 0.01 to work around logarithm of zero\n",
      "    \n",
      "    if word in j_dict: #If the word exists in the j group\n",
      "        j_p[word] = j_dict[word]/float(sum(j_dict.values())) #Compute the probability of that word occurrring in the j group\n",
      "    else: #If the word does not exist in the j group\n",
      "        j_p[word] = 0 #Assign a zero probability\n",
      "        \n",
      "    j_p[word] = 0.99*j_p[word]+0.01*corpus_p[word] #Use the \"fudge factor\" of alpha = 0.01 to work around logarithm of zero\n",
      "    \n",
      "    if word in k_dict: #If the word exists in the k group\n",
      "        k_p[word] = k_dict[word]/float(sum(k_dict.values())) #Compute the probability of that word occurrring in the k group\n",
      "    else: #If the word does not exist in the k group\n",
      "        k_p[word] = 0 #Assign a zero probability\n",
      "        \n",
      "    k_p[word] = 0.99*k_p[word]+0.01*corpus_p[word] #Use the \"fudge factor\" of alpha = 0.01 to work around logarithm of zero\n",
      "     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shannon_entropy_i = -1.0*sum(p*math.log(p,2) for p in i_p.values()) #Compute the Shannon entropy in communicating within the i field\n",
      "shannon_entropy_j = -1.0*sum(p*math.log(p,2) for p in j_p.values()) #Compute the Shannon entropy in communicating within the j field\n",
      "shannon_entropy_k = -1.0*sum(p*math.log(p,2) for p in k_p.values()) #Compute the Shannon entropy in communicating within the k field\n",
      "\n",
      "cross_entropy_itoj = -1.0*sum(i_p[word]*math.log(j_p[word],2) for word in corpus_p.keys()) #Compute the cross entropy in communicating between the i and j fields (i to j)\n",
      "cross_entropy_itok = -1.0*sum(i_p[word]*math.log(k_p[word],2) for word in corpus_p.keys()) #Compute the cross entropy in communicating between the i and k fields (i to k)\n",
      "cross_entropy_jtoi = -1.0*sum(j_p[word]*math.log(i_p[word],2) for word in corpus_p.keys()) #Compute the cross entropy in communicating between the j and i fields (j to i)\n",
      "cross_entropy_jtok = -1.0*sum(j_p[word]*math.log(k_p[word],2) for word in corpus_p.keys()) #Compute the cross entropy in communicating between the j and k fields (j to k)\n",
      "cross_entropy_ktoi = -1.0*sum(k_p[word]*math.log(i_p[word],2) for word in corpus_p.keys()) #Compute the cross entropy in communicating between the k and i fields (k to i)\n",
      "cross_entropy_ktoj = -1.0*sum(k_p[word]*math.log(j_p[word],2) for word in corpus_p.keys()) #Compute the cross entropy in communicating between the k and j fields (k to j)\n",
      "\n",
      "efficiency_itoi = shannon_entropy_i/shannon_entropy_i #Compute the efficiency of communication (from i to i)\n",
      "efficiency_itoj = shannon_entropy_i/cross_entropy_itoj #Compute the efficiency of communication (from i to j)\n",
      "efficiency_itok = shannon_entropy_i/cross_entropy_itok #Compute the efficiency of communication (from i to k)\n",
      "efficiency_jtoj = shannon_entropy_j/shannon_entropy_j #Compute the efficiency of communication (from j to j)\n",
      "efficiency_jtoi = shannon_entropy_j/cross_entropy_jtoi #Compute the efficiency of communication (from j to i)\n",
      "efficiency_jtok = shannon_entropy_j/cross_entropy_jtok #Compute the efficiency of communication (from j to k)\n",
      "efficiency_ktok = shannon_entropy_i/shannon_entropy_i #Compute the efficiency of communication (from k to k)\n",
      "efficiency_ktoi = shannon_entropy_k/cross_entropy_ktoi #Compute the efficiency of communication (from k to i)\n",
      "efficiency_ktoj = shannon_entropy_k/cross_entropy_ktoj #Compute the efficiency of communication (from k to j)\n",
      "\n",
      "jargon_dist_itoi = 1-efficiency_itoi #Compute the jargon distance (from i to i)\n",
      "jargon_dist_itoj = 1-efficiency_itoj #Compute the jargon distance (from i to j)\n",
      "jargon_dist_itok = 1-efficiency_itok #Compute the jargon distance (from i to k)\n",
      "jargon_dist_jtoj = 1-efficiency_jtoj #Compute the jargon distance (from j to j)\n",
      "jargon_dist_jtoi = 1-efficiency_jtoi #Compute the jargon distance (from j to i)\n",
      "jargon_dist_jtok = 1-efficiency_jtok #Compute the jargon distance (from j to k)\n",
      "jargon_dist_ktok = 1-efficiency_ktok #Compute the jargon distance (from k to k)\n",
      "jargon_dist_ktoi = 1-efficiency_ktoi #Compute the jargon distance (from k to i)\n",
      "jargon_dist_ktoj = 1-efficiency_ktoj #Compute the jargon distance (from k to j)\n",
      "\n",
      "print \"Shannon entropy for communication within the first field: \"+str(shannon_entropy_i)\n",
      "print \"Shannon entropy for communication within the second field: \"+str(shannon_entropy_j)\n",
      "print \"Shannon entropy for communication within the third field: \"+str(shannon_entropy_k)\n",
      "\n",
      "print \"\\nCross entropy for communication from first to second: \"+str(cross_entropy_itoj)\n",
      "print \"Cross entropy for communication from first to third: \"+str(cross_entropy_itok)\n",
      "print \"Cross entropy for communication from second to first: \"+str(cross_entropy_jtoi)\n",
      "print \"Cross entropy for communication from second to third: \"+str(cross_entropy_jtok)\n",
      "print \"Cross entropy for communication from third to first: \"+str(cross_entropy_ktoi)\n",
      "print \"Cross entropy for communication from third to second: \"+str(cross_entropy_ktoj)\n",
      "\n",
      "print \"\\nEfficiency for communication from first to first: \"+str(efficiency_itoi)\n",
      "print \"Efficiency for communication from first to second: \"+str(efficiency_itoj)\n",
      "print \"Efficiency for communication from first to third: \"+str(efficiency_itok)\n",
      "print \"Efficiency for communication from second to second: \"+str(efficiency_jtoj)\n",
      "print \"Efficiency for communication from second to first: \"+str(efficiency_jtoi)\n",
      "print \"Efficiency for communication from second to third: \"+str(efficiency_jtok)\n",
      "print \"Efficiency for communication from third to third: \"+str(efficiency_ktok)\n",
      "print \"Efficiency for communication from third to first: \"+str(efficiency_ktoi)\n",
      "print \"Efficiency for communication from third to second: \"+str(efficiency_ktoj)\n",
      "\n",
      "\n",
      "print \"\\nJargon distance from first to first: \"+str(jargon_dist_itoi)\n",
      "print \"Jargon distance from first to second: \"+str(jargon_dist_itoj)\n",
      "print \"Jargon distance from first to third: \"+str(jargon_dist_itok)\n",
      "print \"Jargon distance from second to second: \"+str(jargon_dist_jtoj)\n",
      "print \"Jargon distance from second to first: \"+str(jargon_dist_jtoi)\n",
      "print \"Jargon distance from second to third: \"+str(jargon_dist_jtok)\n",
      "print \"Jargon distance from third to third: \"+str(jargon_dist_ktok)\n",
      "print \"Jargon distance from third to first: \"+str(jargon_dist_ktoi)\n",
      "print \"Jargon distance from third to secnd: \"+str(jargon_dist_ktoj)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shannon entropy for communication within the first field: 8.71656362171\n",
        "Shannon entropy for communication within the second field: 8.3383857899\n",
        "Shannon entropy for communication within the third field: 7.48086199931\n",
        "\n",
        "Cross entropy for communication from first to second: 15.5646509998\n",
        "Cross entropy for communication from first to third: 15.5417721533\n",
        "Cross entropy for communication from second to first: 15.4270867089\n",
        "Cross entropy for communication from second to third: 15.67254416\n",
        "Cross entropy for communication from third to first: 14.8145635066\n",
        "Cross entropy for communication from third to second: 15.3683395096\n",
        "\n",
        "Efficiency for communication from first to first: 1.0\n",
        "Efficiency for communication from first to second: 0.560023069057\n",
        "Efficiency for communication from first to third: 0.560847471945\n",
        "Efficiency for communication from second to second: 1.0\n",
        "Efficiency for communication from second to first: 0.540502944413\n",
        "Efficiency for communication from second to third: 0.532037791998\n",
        "Efficiency for communication from third to third: 1.0\n",
        "Efficiency for communication from third to first: 0.504966750858\n",
        "Efficiency for communication from third to second: 0.486771000511\n",
        "\n",
        "Jargon distance from first to first: 0.0\n",
        "Jargon distance from first to second: 0.439976930943\n",
        "Jargon distance from first to third: 0.439152528055\n",
        "Jargon distance from second to second: 0.0\n",
        "Jargon distance from second to first: 0.459497055587\n",
        "Jargon distance from second to third: 0.467962208002\n",
        "Jargon distance from third to third: 0.0\n",
        "Jargon distance from third to first: 0.495033249142\n",
        "Jargon distance from third to secnd: 0.513228999489\n"
       ]
      }
     ],
     "prompt_number": 138
    }
   ],
   "metadata": {}
  }
 ]
}