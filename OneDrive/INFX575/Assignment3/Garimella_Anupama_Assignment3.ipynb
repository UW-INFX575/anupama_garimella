{
 "metadata": {
  "name": "",
  "signature": "sha256:2b9578b09aa322bbbbd2d0ac195789788896d42f4653355107ca3142fcdaf07d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import required packages\n",
      "import numpy as np\n",
      "import lda\n",
      "import nltk \n",
      "import gensim\n",
      "from nltk.corpus import stopwords\n",
      "import nltk.data\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Begin LDA\n",
      "\n",
      "documents = [] #Create an empty list to store documents\n",
      "for i in range(0,10): #Iterate over each file\n",
      "    filename = str(6334220+i) #Generate the file name dynamically for each iteration\n",
      "    \n",
      "    #Open and read file\n",
      "    f = open(\"C:\\\\Users\\\\Anupama\\\\OneDrive\\\\INFX575\\\\Assignment2\\\\textfiles\\\\\"+filename+\".txt\", \"r\")\n",
      "    textfile = f.read()\n",
      "    documents.append(textfile)\n",
      "\n",
      "stoplist = stopwords.words('english') #Store stop words in a list\n",
      "\n",
      "#Use the gensim package to perform LDA\n",
      "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
      "dictionary = gensim.corpora.Dictionary(texts)\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, update_every=1, chunksize=10000, passes=20)\n",
      "print \"LDA topics:\"\n",
      "lda.print_topics(5)\n",
      "#End LDA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LDA topics:\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "[u'0.054*said + 0.031*assembly + 0.029*claim + 0.022*wherein + 0.017*means + 0.017*inner + 0.015*bathtub + 0.015*seat + 0.014*rotary + 0.014*rails',\n",
        " u'0.036*portion + 0.035*said + 0.033*wherein + 0.030*bag + 0.030*sleeping + 0.026*claim + 0.026*claimed + 0.024*body + 0.021*least + 0.021*main',\n",
        " u'0.071*said + 0.036*first + 0.030*wherein + 0.030*second + 0.030*claim + 0.021*edge + 0.020*defined + 0.018*generally + 0.016*comprises + 0.015*semiconductor',\n",
        " u'0.073*end + 0.062*first + 0.054*protector + 0.049*second + 0.042*strap + 0.027*section + 0.022*front + 0.019*support + 0.016*side + 0.016*positioned',\n",
        " u'0.054*said + 0.044*support + 0.033*member + 0.031*wherein + 0.029*plurality + 0.028*faucet + 0.027*, + 0.024*members + 0.024*tightening + 0.022*claim']"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Begin jargon distance implementation\n",
      "documents = [] #Create an empty list of documents for the toy data set\n",
      "\n",
      "for i in range(1,6): #Iterate over each file\n",
      "    filename = \"doc\"+str(i) #Generate the file name dynamically for each iteration\n",
      "    \n",
      "    #Open and read file\n",
      "    f = open(\"C:\\\\Users\\\\Anupama\\\\OneDrive\\\\INFX575\\\\Assignment3\\\\\"+filename+\".txt\", \"r\")\n",
      "    textfile = f.read()\n",
      "    documents.append(textfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i_group = documents[0:2] #Cluster documents about food together into the \"i\" field\n",
      "j_group = documents[2:6] #Cluster documents about animals together into the \"j\" field\n",
      "corpus = documents #Create a corpus containing all documents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenizer = RegexpTokenizer(r'\\w+') #Initialize a tokenizer\n",
      "i_tokens = tokenizer.tokenize(i_group[0])+tokenizer.tokenize(i_group[1]) #Convert the i group into a flat list of tokens\n",
      "i_tokens = [word.lower() for word in i_tokens if word not in stoplist] #Remove stop words from the i group\n",
      "j_tokens = tokenizer.tokenize(j_group[0])+tokenizer.tokenize(j_group[1])+tokenizer.tokenize(j_group[2]) #Convert the j group into a flaat list of tokens\n",
      "j_tokens = [word.lower() for word in j_tokens if word not in stoplist] #Remove stop words from the j group\n",
      "corpus_tokens = i_tokens+j_tokens #Combine the cleaned up tokens together into a corpus of tokens\n",
      "\n",
      "i_dict = {x:i_tokens.count(x) for x in i_tokens} #Create a word frequency dictionary for the i cluster\n",
      "j_dict = {x:j_tokens.count(x) for x in j_tokens} #Create a word frequency dictionary for the j cluster\n",
      "corpus_dict = {x:corpus_tokens.count(x) for x in corpus_tokens} #Create a word frequency dictionary for the whole corpus\n",
      "\n",
      "i_p={} #Create an empty dictionary to store the probability distributions for the words in the i cluster\n",
      "j_p={} #Create an empty dictionary to store the probability distributions for the words in the j cluster\n",
      "corpus_p={} #Create an empty dictionary to store the probability distributions for the words in the whole corpus\n",
      "\n",
      "for word in corpus_dict.keys(): #For each word in the corpus\n",
      "    \n",
      "    corpus_p[word] = corpus_dict[word]/float(sum(corpus_dict.values())) #Compute the probability of that word occurring in the corpus\n",
      "    \n",
      "    if word in i_dict: #If the word exists in the i group\n",
      "        i_p[word] = i_dict[word]/float(sum(i_dict.values())) #Compute the probability of that word occurrring in the i group\n",
      "    else: #If the word does not exist in the i group\n",
      "        i_p[word] = 0 #Assign a zero probability\n",
      "    \n",
      "    i_p[word] = 0.95*i_p[word]+0.01*corpus_p[word] #Use the \"fudge factor\" of alpha = 0.01 to work around logarithm of zero\n",
      "    \n",
      "    if word in j_dict: #If the word exists in the j group\n",
      "        j_p[word] = j_dict[word]/float(sum(j_dict.values())) #Compute the probability of that word occurrring in the j group\n",
      "    else: #If the word does not exist in the j group\n",
      "        j_p[word] = 0 #Assign a zero probability\n",
      "        \n",
      "    j_p[word] = 0.95*j_p[word]+0.01*corpus_p[word] #Use the \"fudge factor\" of alpha = 0.01 to work around logarithm of zero"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shannon_entropy_i = -1.0*sum(p*math.log(p,2) for p in i_p.values()) #Compute the Shannon entropy in communicating within the i field\n",
      "shannon_entropy_j = -1.0*sum(p*math.log(p,2) for p in j_p.values()) #Compute the Shannon entropy in communicating within the j field\n",
      "cross_entropy_itoj = -1.0*sum(i_p[word]*math.log(j_p[word],2) for word in corpus_p.keys()) #Compute the cross entropy in communicating between the i and j fields (i to j)\n",
      "cross_entropy_jtoi = -1.0*sum(j_p[word]*math.log(i_p[word],2) for word in corpus_p.keys()) #Compute the cross entropy in communicating between the i and j fields (j to i)\n",
      "efficiency_itoj = shannon_entropy_i/cross_entropy_itoj #Compute the efficiency of communication (from i to j)\n",
      "efficiency_jtoi = shannon_entropy_j/cross_entropy_jtoi #Compute the efficiency of communication (from j to i)\n",
      "jargon_dist_itoj = 1-efficiency_itoj #Compute the jargon distance (from i to j)\n",
      "jargon_dist_jtoi = 1-efficiency_jtoi #Compute the jargon distance (from j to i)\n",
      "\n",
      "print \"Shannon entropy for communication within the i field: \"+str(shannon_entropy_i)\n",
      "print \"Shannon entropy for communication within the j field: \"+str(shannon_entropy_j)\n",
      "print \"Cross entropy for communication from i to j: \"+str(cross_entropy_itoj)\n",
      "print \"Cross entropy for communication from j to i: \"+str(cross_entropy_jtoi)\n",
      "print \"Efficiency for communication from i to j: \"+str(efficiency_itoj)\n",
      "print \"Efficiency for communication from j to i: \"+str(efficiency_jtoi)\n",
      "print \"Jargon distance from i to j: \"+str(jargon_dist_itoj)\n",
      "print \"Jargon distance from j to i: \"+str(jargon_dist_jtoi)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shannon entropy for communication within the i field: 3.25114628762\n",
        "Shannon entropy for communication within the j field: 3.60972741016\n",
        "Cross entropy for communication from i to j: 9.9762806041\n",
        "Cross entropy for communication from j to i: 10.1345457853\n",
        "Efficiency for communication from i to j: 0.325887614497\n",
        "Efficiency for communication from j to i: 0.35618048274\n",
        "Jargon distance from i to j: 0.674112385503\n",
        "Jargon distance from j to i: 0.64381951726\n"
       ]
      }
     ],
     "prompt_number": 81
    }
   ],
   "metadata": {}
  }
 ]
}