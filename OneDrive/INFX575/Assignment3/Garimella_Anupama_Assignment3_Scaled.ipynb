{
 "metadata": {
  "name": "",
  "signature": "sha256:123e35f6bc6f03b00319d1f8a284098aed6ddca3fd00ec64ed564370811436e7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import required packages\n",
      "import numpy as np\n",
      "import lda\n",
      "import nltk \n",
      "import gensim\n",
      "from nltk.corpus import stopwords\n",
      "import nltk.data\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "import math\n",
      "import time\n",
      "from scipy.cluster.hierarchy import dendrogram, linkage\n",
      "from matplotlib.pyplot import show"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_time = time.time() #Start timer\n",
      "import csv\n",
      "#Import documents\n",
      "with open(\"C:\\\\Users\\\\Anupama\\\\OneDrive\\\\INFX575\\\\Assignment3\\\\abstracts2.txt\\\\abstracts2.txt\") as f:\n",
      "    reader = csv.reader(f, delimiter=\"\\t\")\n",
      "    abstracts = list(reader)\n",
      "    \n",
      "#Set up a document dictionary that maps document id to document\n",
      "abstracts_dict = {t[0]:t[1] for t in abstracts}\n",
      "\n",
      "#Import the group mappings\n",
      "with open(\"C:\\\\Users\\\\Anupama\\\\OneDrive\\\\INFX575\\\\Assignment3\\\\groups2.txt\\\\groups2.txt\") as f:\n",
      "    reader = csv.reader(f, delimiter=\"\\t\")\n",
      "    groups = list(reader)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Set up a group dictionary that maps document to group number\n",
      "groups_dict = {t[0]:t[1] for t in groups[1:]}\n",
      "\n",
      "#Create clusters\n",
      "clusters = [] \n",
      "for i in range (0,10): #Create 10 clusters. For each cluster\n",
      "    clusters.append(list(key for key, value in groups_dict.iteritems() if int(value) == i+1)) #Fetch all document ids that belong to the current cluster number\n",
      "    clusters[i] = [abstracts_dict[x] for x in clusters[i]] #Replace the document id with the actual document\n",
      "    clusters[i] = [doc for doc in clusters[i] if doc != 'null'] #Remove null documents       \n",
      "    \n",
      "#Create a corpus comprising of all documents\n",
      "corpus = list(abstract for cluster in clusters for abstract in cluster)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Initialize a stop list\n",
      "stoplist = [\"all\",\"just\",\"being\",\"over\",\"both\",\"through\",\"yourselves\",\"its\",\"before\",\"herself\",\"had\",\"should\",\"to\",\"only\",\"under\",\"ours\",\"has\",\"do\",\"them\",\"his\",\"very\",\"they\",\"not\",\"during\",\"now\",\"him\",\"nor\",\"did\",\"this\",\"she\",\"each\",\"further\",\"where\",\"few\",\"because\",\"doing\",\"some\",\"are\",\"our\",\"ourselves\",\"out\",\"what\",\"for\",\"while\",\"does\",\"above\",\"between\",\"t\",\"be\",\"we\",\"who\",\"were\",\"here\",\"hers\",\"by\",\"on\",\"about\",\"of\",\"against\",\"s\",\"or\",\"own\",\"into\",\"yourself\",\"down\",\"your\",\"from\",\"her\",\"their\",\"there\",\"been\",\"whom\",\"too\",\"themselves\",\"was\",\"until\",\"more\",\"himself\",\"that\",\"but\",\"don\",\"with\",\"than\",\"those\",\"he\",\"me\",\"myself\",\"these\",\"up\",\"will\",\"below\",\"can\",\"theirs\",\"my\",\"and\",\"then\",\"is\",\"am\",\"it\",\"an\",\"as\",\"itself\",\"at\",\"have\",\"in\",\"any\",\"if\",\"again\",\"no\",\"when\",\"same\",\"how\",\"other\",\"which\",\"you\",\"after\",\"most\",\"such\",\"why\",\"a\",\"off\",\"i\",\"yours\",\"so\",\"the\",\"having\",\"once\"]\n",
      "tokenizer = RegexpTokenizer(r'\\w+') #Initialize a tokenizer\n",
      "\n",
      "tokens = [] #Create a list of lists to store the tokens in each cluster\n",
      "for i in range (0,10): #For each token set (There will be 10, one for each group)\n",
      "    tokens.append(tokenizer.tokenize(\" \".join(clusters[i]))) #Tokenize each document and lump all the tokens in all documents in the current cluster together\n",
      "    tokens[i] = [word.lower() for word in tokens[i] if word not in stoplist] #Convert all words to lower case and remove stop words\n",
      "      \n",
      "corpus_tokens = list(token for tokenset in tokens for token in tokenset) #Combine all tokens in all clusters into what will be the corpus tokens\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq_dict = [] #Create a list where we will store word frequency diustrubutions for each cluster\n",
      "for i in range (0,10): #For each cluster\n",
      "    freq_dict.append(nltk.FreqDist(tokens[i])) #Compute the word frequency distrubution for the current cluster\n",
      "    \n",
      "corpus_dict = nltk.FreqDist(corpus_tokens) #Compute the word frequency distribution for the corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_sum = float(sum(corpus_dict.values())) #Compute the total number of words in the corpus\n",
      "for word in corpus_dict.keys(): #For each word in the corpus\n",
      "    corpus_dict[word] = corpus_dict[word]/corpus_sum #Calculate th probability of finding the current word in the corpus\n",
      "    \n",
      "for grp_dict in freq_dict: #for each cluster\n",
      "    grp_sum = float(sum(grp_dict.values())) #Compute the total number of words in the current cluster\n",
      "    for word in corpus_dict: #For each word in the corpus\n",
      "        try:\n",
      "           grp_dict[word] = 0.99*(grp_dict[word]/grp_sum)+0.01*corpus_dict[word] #Incorporate the fudge factor\n",
      "        except KeyError, e: #If the current word is not found in the current cluster\n",
      "           grp_dict[word] = 0.01*corpus_dict[word] #Use a probability of zero, and incorporate the fudge factor       "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Compute Shannon entropies\n",
      "shannon_entropy = [] \n",
      "for i in range (0,10):\n",
      "    shannon_entropy.append(-1.0*sum(p*math.log(p,2) for p in freq_dict[i].values()))\n",
      "    \n",
      "#Compute cross entropies and store them in a 10x10 matrix\n",
      "cross_entropy = np.zeros((10,10))\n",
      "for i in range (0,10):\n",
      "    for j in range (0,10):\n",
      "       cross_entropy[i][j] = -1.0*sum(freq_dict[i][word]*math.log(freq_dict[j][word],2) for word in corpus_dict.keys()) \n",
      "        \n",
      "#Compute the efficiency of communication, jargon distances between individual clusters and store in 10x10 matrices\n",
      "efficiency = np.zeros((10,10))       \n",
      "jargon_dist = np.zeros((10,10))\n",
      "for i in range (0,10):\n",
      "    for j in range (0,10):\n",
      "        efficiency[i][j] = shannon_entropy[i]/cross_entropy[i][j]\n",
      "        jargon_dist[i][j] = 1 - efficiency[i][j]\n",
      "        if jargon_dist[i][j] < 0.001:\n",
      "            jargon_dist[i][j] = 0          \n",
      "            \n",
      "print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"           "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time taken: 50.7 seconds\n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z = linkage(jargon_dist, method = 'average') #Prepare a linkage matrix\n",
      "#build a dendrogram\n",
      "dendrogram(Z, labels = ['Ecology and Evolution', 'Molecular and Cell Biology', 'Economics', 'Sociology', 'Probability and Statistics','Organizational and marketing', 'Law', 'Anthropology', 'Political Science', 'Education'])\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    }
   ],
   "metadata": {}
  }
 ]
}